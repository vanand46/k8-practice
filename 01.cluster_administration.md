# Upgrading Kubernetes and Backing Up etcd

This document provides a step-by-step guide on how to upgrade a Kubernetes cluster created with `kubeadm` and how to back up the etcd cluster data.

## Upgrading Control Plane's Kubernetes Version

This section covers the process of upgrading the Kubernetes control plane.

**Reference:**
*   **Master Upgrade:** [https://v1-30.docs.kubernetes.io/docs/tasks/administer-cluster/kubeadm/kubeadm-upgrade/](https://v1-30.docs.kubernetes.io/docs/tasks/administer-cluster/kubeadm/kubeadm-upgrade/)
*   **Worker Node Upgrade:** [https://v1-30.docs.kubernetes.io/docs/tasks/administer-cluster/kubeadm/upgrading-linux-nodes/](https://v1-30.docs.kubernetes.io/docs/tasks/administer-cluster/kubeadm/upgrading-linux-nodes/)

### Steps to be followed:
1.  Upgrade control plane to v1.30.9
2.  Upgrade worker node to v1.30.9
3.  Validate upgrade, by creating a pod

### Step 1: Upgrade control plane to v1.30.9

#### Fix for apt update issue: (Not Always, may be issue with this environment)
First, address potential `apt` repository key issues.

```bash
sudo apt-key del 234654DA9A296436

curl -fsSL https://pkgs.k8s.io/core:/stable:/v1.30/deb/Release.key | sudo gpg --dearmor -o /usr/share/keyrings/k8s-archive-keyring.gpg

echo "deb [signed-by=/usr/share/keyrings/k8s-archive-keyring.gpg] https://pkgs.k8s.io/core:/stable:/v1.30/deb/ ./" | sudo tee /etc/apt/sources.list.d/kubernetes.list

sudo apt update
```

#### Upgrade kubeadm

```bash
sudo apt-cache madison kubeadm
```
Pick the latest version from the previous output. We are choosing `1.30.9-1.1` for the upgrade.

```bash
sudo apt-mark unhold kubeadm
sudo apt-get update
sudo apt-get install -y kubeadm='1.30.9-1.1'
sudo apt-mark hold kubeadm
```

Check the current version of kubeadm.

```bash
kubeadm version
```

Plan the upgrade.

```bash
sudo kubeadm upgrade plan
```

Apply the upgrade.

```bash
sudo kubeadm upgrade apply v1.30.9
```

Drain the master node.

```bash
kubectl drain master.example.com --ignore-daemonsets
```

Upgrade kubelet and kubectl on the master node.

```bash
sudo apt-mark unhold kubelet kubectl
sudo apt-get update
sudo apt-get install -y kubelet='1.30.9-1.1' kubectl='1.30.9-1.1'
sudo apt-mark hold kubelet kubectl
```

Reload the systemd manager configuration and restart kubelet.

```bash
sudo systemctl daemon-reload
sudo systemctl restart kubelet
```

Uncordon the master node to make it schedulable again.

```bash
kubectl uncordon master.example.com
kubectl get nodes
```

### Step 2: Upgrade worker node to v1.30.9

**On the Worker node:**

Update the package list and upgrade kubeadm.

```bash
sudo apt-get update
sudo apt-mark unhold kubeadm
sudo apt-get update
sudo apt-get install -y kubeadm='1.30.9-1.1'
sudo apt-mark hold kubeadm
```

Check the kubeadm version.
```bash
kubeadm version
```

Upgrade the node configuration.

```bash
sudo kubeadm upgrade node
```

**On the Master node:**

Drain the worker node.
```bash
kubectl drain worker-node-1.example.com --ignore-daemonsets --delete-emptydir-data
```
**Note:** While executing the above drain command, if there are pods that cannot be deleted, you might see the error message: `cannot delete Pods declare no controller (use --force to override)`.

In that case, use the below command:
```bash
kubectl drain worker-node-1.example.com --ignore-daemonsets --delete-emptydir-data --force
```

**On the Worker node:**

Upgrade kubelet and kubectl.
```bash
sudo apt-mark unhold kubelet kubectl
sudo apt-get update
sudo apt-get install -y kubelet='1.30.9-1.1' kubectl='1.30.9-1.1'
sudo apt-mark hold kubelet kubectl
```

Reload the systemd manager configuration and restart kubelet.
```bash
sudo systemctl daemon-reload
sudo systemctl restart kubelet
```

**On the Master node:**

Uncordon the worker node.
```bash
kubectl uncordon worker-node-1.example.com
kubectl get nodes
```

Cluster and Worker nodes are successfully upgraded to v1.30.9.

### Step 3: Validate cluster upgrade, by creating a pod

**On the Master node:**

Create a test pod to validate that the cluster is functioning correctly.
```bash
kubectl run test-pod --image nginx --port 80
```

Check the status of the pod.
```bash
kubectl get pods -o wide
```

## ETCD - Backup

This section describes how to back up your etcd data.

**Reference:** [https://kubernetes.io/docs/tasks/administer-cluster/configure-upgrade-etcd/#restoring-an-etcd-cluster](https://kubernetes.io/docs/tasks/administer-cluster/configure-upgrade-etcd/#restoring-an-etcd-cluster)

### Step 1: Install etcd-client
```bash
sudo snap install etcd
sudo apt install etcd-client
sudo chmod a+rw -R /etc/kubernetes/pki
```

### Step 2: Backup data
```bash
sudo ETCDCTL_API=3 etcdctl snapshot save etcd_backup.db \
--endpoints https://127.0.0.1:2379 \
--cert=/etc/kubernetes/pki/etcd/server.crt \
--key=/etc/kubernetes/pki/etcd/server.key \
--cacert=/etc/kubernetes/pki/etcd/ca.crt
```

### Step 3: Verify
```bash
sudo ETCDCTL_API=3 etcdctl --write-out=table snapshot status etcd_backup.db \
--endpoints https://127.0.0.1:2379 \
--cert=/etc/kubernetes/pki/etcd/server.crt \
--key=/etc/kubernetes/pki/etcd/server.key \
--cacert=/etc/kubernetes/pki/etcd/ca.crt
```